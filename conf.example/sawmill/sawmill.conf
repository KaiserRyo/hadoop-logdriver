## Sawmill config file

##### Global Configuration #####
## These are global configuration options.  None of them are required.

## List of locations for hadoop config files
# hadoop.config.paths = file:///etc/hadoop/conf/core-site.xml file:///etc/hadoop/conf/hdfs-site.xml

## Kerberos credentials, if using Kerberos
# kerberos.principal = myuser@MYDOMAIN
# kerberos.keytab = /etc/logdriver/conf.local/myuser.keytab

## Number of threads for running the parser/writers.  Default is number of
## available processors * 2
## Suggested value is equal to the number of paths
# threadpool.size = 100

## Default value for all paths' tcp.max.line.length
# default.tcp.max.line.length = 4096


##### Path List #####
## Space separated list of paths to use.  You must add a path to this list,
## and also configure it below in order to use it.
paths = path1


##### Path Configurations #####
## Each configuration will start with 'path.<pathname>'.
## Only port and file.path.template are required.

## Name your path - it will be used in JMX monitoring
# path.path1.name = my path name

## Max line length you will accept.
# path.path1.tcp.max.line.length = 4096

## TCP read buffer size
# path.path1.tcp.read.buffer.size = 2048

## TCP bind address.  Defaults to 0.0.0.0
# path.path1.bind.address = 0.0.0.0

## Port to listen on.  Required value.
path.path1.port = 10000

## Which character set to use.  See 'Canonical Name for java.nio API' at
## http://docs.oracle.com/javase/6/docs/technotes/guides/intl/encoding.doc.html
## for the options.  Default is UTF-8.
# path.path1.charset = UTF-8

## Number of output buckets to use for this path.  Generally, each bucket can
## handle about 10,000 lines per second, so you should only have to up this
## above 1 if you are doing more than that on one port.
# path.path1.output.buckets = 1

## Size of the queue for this path, in number of events.
# path.path1.queue.capacity = 100

## Path template. Required.
## Valid replacement strings are
##   %y  The year from the line's timestamp
##   %M  The month from the line's timestamp
##   %d  The day from the line's timestamp
##   %H  The hour from the line's timestamp
##   %m  The minute from the line's timestamp
##   %s  The second from the line's timestamp
##   %l  The sawmill server's hostname (not the name in the log)
path.path1.file.path.template = hdfs://namenode.mydomain:8020/service/1/myservice/logs/%y%M%d/%H/mycomponent/incoming/%l/sawmill

## How often to rotate output files, in seconds
# path.path1.file.rotate.interval = 600

## The user we are writing as.  Ensure you have permission to impersonate this
## user. Only required if using Kerberos
# path.path1.hdfs.proxy.user = myserviceuser

## HDFS block size to use when writing
# path.path1.hdfs.block.size = 268435456

## HDFS replicas for written files
# path.path1.hdfs.replicas = 3

## HDFS writer buffer size
# path.path1.hdfs.buffer.size = 4096

## Deflate level to use when writing Boom files
# path.path1.boom.deflate.level = 6

## Avro sync interval for Boom files
# path.path1.boom.sync.interval = 2097152
